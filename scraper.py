"""
ë…¸ë™ì•ˆì „ë³´ê±´ ì¼ì¼ ë™í–¥ ë¸Œë¦¬í•‘ ì‹œìŠ¤í…œ
ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict
import time
import os
import subprocess

# PlaywrightëŠ” ì„ íƒì ìœ¼ë¡œ import
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
    
    # Streamlit Cloudì—ì„œ ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì € ì„¤ì¹˜
    if not os.path.exists(os.path.expanduser("~/.cache/ms-playwright")):
        print("ğŸ”„ Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì¤‘...")
        try:
            subprocess.run(["playwright", "install", "chromium", "--with-deps"], 
                         check=True, capture_output=True)
            print("âœ… Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
            PLAYWRIGHT_AVAILABLE = False
            
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸ Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë¶€ ì‚¬ì´íŠ¸ ìˆ˜ì§‘ì´ ì œí•œë©ë‹ˆë‹¤.")


class SafetyNewsScraper:
    """ë…¸ë™ì•ˆì „ë³´ê±´ ê´€ë ¨ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        self.today = datetime.now().strftime("%Y-%m-%d")
        self.results = {
            'moel_press': [],          # ê³ ìš©ë…¸ë™ë¶€ ë³´ë„ìë£Œ
            'kosha_notice': [],        # ì‚°ì—…ì•ˆì „í¬í„¸ ê³µì§€ì‚¬í•­
            'major_accident': [],      # ì¤‘ëŒ€ì¬í•´ ë°œìƒì•Œë¦¼
            'labor_news': [],          # ë§¤ì¼ë…¸ë™ë‰´ìŠ¤
            'bigkinds_news': []        # Bigkinds ë‰´ìŠ¤ ê²€ìƒ‰
        }
    
    def scrape_moel_press_release(self):
        """ê³ ìš©ë…¸ë™ë¶€ ë³´ë„ìë£Œ ìˆ˜ì§‘"""
        print("ğŸ“„ ê³ ìš©ë…¸ë™ë¶€ ë³´ë„ìë£Œ ìˆ˜ì§‘ ì¤‘...")
        url = "https://www.moel.go.kr/news/enews/report/enewsList.do"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=30, headers=headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í…Œì´ë¸”ì—ì„œ ìµœê·¼ ê²Œì‹œë¬¼ ì¶”ì¶œ
            table = soup.find('table')
            if not table:
                print("  âš ï¸ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return
            
            rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
            
            for row in rows[:15]:  # ìµœê·¼ 15ê°œ ì²´í¬
                try:
                    cols = row.find_all('td')
                    if len(cols) < 4:
                        continue
                    
                    # ì œëª© ì—´ ì°¾ê¸° (ë³´í†µ 2ë²ˆì§¸ td)
                    title_col = None
                    for col in cols:
                        link_tag = col.find('a')
                        if link_tag:
                            title_col = col
                            break
                    
                    if not title_col:
                        continue
                    
                    link_tag = title_col.find('a')
                    title = link_tag.get_text(strip=True)
                    href = link_tag.get('href', '')
                    
                    # ë§í¬ ì²˜ë¦¬
                    if href.startswith('http'):
                        link = href
                    else:
                        link = "https://www.moel.go.kr/news/enews/report/" + href
                    
                    # ë‚ ì§œ ì¶”ì¶œ (ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ ì—´)
                    date = cols[-2].get_text(strip=True) if len(cols) >= 2 else ''
                    
                    # ì•ˆì „ë³´ê±´ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                    keywords = ['ì•ˆì „', 'ì‚°ì¬', 'ì¤‘ëŒ€ì¬í•´', 'ë³´ê±´', 'ì¬í•´', 'ì‚¬ê³ ', 'ìœ„í—˜', 'ê·¼ë¡œ', 'ë…¸ë™']
                    if any(keyword in title for keyword in keywords):
                        self.results['moel_press'].append({
                            'title': title,
                            'date': date,
                            'link': link,
                            'source': 'ê³ ìš©ë…¸ë™ë¶€'
                        })
                except Exception as e:
                    continue
            
            print(f"  âœ… {len(self.results['moel_press'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def scrape_kosha_with_playwright(self):
        """ì‚°ì—…ì•ˆì „í¬í„¸ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ (Playwright ì‚¬ìš©)"""
        print("ğŸ“„ ì‚°ì—…ì•ˆì „í¬í„¸ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ ì¤‘...")
        
        if not PLAYWRIGHT_AVAILABLE:
            print("  âš ï¸ Playwright ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€ (requests ë°©ì‹ìœ¼ë¡œ ì‹œë„)")
            self.scrape_kosha_with_requests()
            return
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = context.new_page()
                
                # íƒ€ì„ì•„ì›ƒ ì§§ê²Œ ì„¤ì • (20ì´ˆ)
                page.set_default_timeout(20000)
                
                print("  â†’ í˜ì´ì§€ ë¡œë”© ì¤‘...")
                try:
                    page.goto("https://portal.kosha.or.kr/community/notice", 
                             wait_until='domcontentloaded', timeout=15000)
                except:
                    print("  âš ï¸ í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼ - ê±´ë„ˆëœ€")
                    browser.close()
                    return
                
                # í…Œì´ë¸” ëŒ€ê¸° (ì§§ì€ ì‹œê°„)
                try:
                    page.wait_for_selector('table', timeout=10000)
                except:
                    print("  âš ï¸ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                    browser.close()
                    return
                
                print("  â†’ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                rows = page.query_selector_all('tbody tr')
                
                count = 0
                for row in rows[:15]:
                    try:
                        tds = row.query_selector_all('td')
                        if len(tds) < 3:
                            continue
                        
                        title_elem = None
                        date_elem = None
                        
                        for td in tds:
                            link = td.query_selector('a')
                            if link and not title_elem:
                                title_elem = link
                        
                        if len(tds) >= 4:
                            date_elem = tds[-2]
                        
                        if title_elem:
                            title = title_elem.inner_text().strip()
                            href = title_elem.get_attribute('href') or ''
                            date = date_elem.inner_text().strip() if date_elem else ''
                            
                            if href.startswith('http'):
                                link = href
                            else:
                                link = "https://portal.kosha.or.kr" + href
                            
                            self.results['kosha_notice'].append({
                                'title': title,
                                'date': date,
                                'link': link,
                                'source': 'ì‚°ì—…ì•ˆì „í¬í„¸'
                            })
                            count += 1
                    except:
                        continue
                
                browser.close()
            
            print(f"  âœ… {len(self.results['kosha_notice'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âš ï¸ ì ‘ì† ë¶ˆê°€ - ê±´ë„ˆëœ€")
    
    def scrape_kosha_with_requests(self):
        """ì‚°ì—…ì•ˆì „í¬í„¸ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ (requests ì‚¬ìš©)"""
        try:
            url = "https://portal.kosha.or.kr/community/notice"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=15, headers=headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œë„
            table = soup.find('table')
            if table:
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                for row in rows[:15]:
                    try:
                        link_tag = row.find('a')
                        if not link_tag:
                            continue
                        
                        title = link_tag.get_text(strip=True)
                        href = link_tag.get('href', '')
                        
                        if href.startswith('http'):
                            link = href
                        else:
                            link = "https://portal.kosha.or.kr" + href
                        
                        # ë‚ ì§œ ì°¾ê¸°
                        tds = row.find_all('td')
                        date = ''
                        for td in tds:
                            text = td.get_text(strip=True)
                            if '.' in text and len(text) < 15:
                                date = text
                                break
                        
                        self.results['kosha_notice'].append({
                            'title': title,
                            'date': date,
                            'link': link,
                            'source': 'ì‚°ì—…ì•ˆì „í¬í„¸'
                        })
                    except:
                        continue
            
            print(f"  âœ… {len(self.results['kosha_notice'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš ï¸ ì ‘ì† ë¶ˆê°€ - ê±´ë„ˆëœ€")
    
    def scrape_major_accidents(self):
        """ì¤‘ëŒ€ì¬í•´ ë°œìƒì•Œë¦¼ ìˆ˜ì§‘"""
        print("ğŸš¨ ì¤‘ëŒ€ì¬í•´ ë°œìƒì•Œë¦¼ ìˆ˜ì§‘ ì¤‘...")
        
        if not PLAYWRIGHT_AVAILABLE:
            print("  âš ï¸ Playwright ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€")
            return
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = context.new_page()
                page.set_default_timeout(20000)
                
                print("  â†’ í˜ì´ì§€ ë¡œë”© ì¤‘...")
                try:
                    page.goto(
                        "https://portal.kosha.or.kr/archive/imprtnDsstrAlrame/CSADV50000/CSADV50000M02",
                        wait_until='domcontentloaded', timeout=15000
                    )
                except:
                    print("  âš ï¸ í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼ - ê±´ë„ˆëœ€")
                    browser.close()
                    return
                
                # ì•½ê°„ë§Œ ëŒ€ê¸°
                page.wait_for_timeout(2000)
                
                # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                print("  â†’ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                selectors = [
                    '.card-list .card-item',
                    'article',
                    '.list-item',
                    '[class*="card"]'
                ]
                
                cards = []
                for selector in selectors:
                    try:
                        cards = page.query_selector_all(selector)
                        if len(cards) > 0:
                            break
                    except:
                        continue
                
                if cards:
                    for card in cards[:5]:
                        try:
                            title = ''
                            title_selectors = ['.card-title', 'h3', 'h4', '.title', 'a']
                            
                            for ts in title_selectors:
                                title_elem = card.query_selector(ts)
                                if title_elem:
                                    title = title_elem.inner_text().strip()
                                    break
                            
                            if not title:
                                title = card.inner_text().strip()[:100]
                            
                            date = ''
                            date_selectors = ['.card-date', '.date', 'time', 'span']
                            for ds in date_selectors:
                                date_elem = card.query_selector(ds)
                                if date_elem:
                                    date_text = date_elem.inner_text().strip()
                                    if len(date_text) > 0 and len(date_text) < 20:
                                        date = date_text
                                        break
                            
                            if title:
                                self.results['major_accident'].append({
                                    'title': title,
                                    'date': date or self.today,
                                    'source': 'ì•ˆì „ë³´ê±´ê³µë‹¨'
                                })
                        except:
                            continue
                
                browser.close()
            
            print(f"  âœ… {len(self.results['major_accident'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âš ï¸ ì ‘ì† ë¶ˆê°€ - ê±´ë„ˆëœ€")
    
    def scrape_labor_news(self):
        """ë§¤ì¼ë…¸ë™ë‰´ìŠ¤ ì•ˆì „ê³¼ ê±´ê°• ì½”ë„ˆ ìˆ˜ì§‘"""
        print("ğŸ“° ë§¤ì¼ë…¸ë™ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            url = "https://www.labortoday.co.kr/news/articleList.html?sc_section_code=S1N7&view_type=sm"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=30, headers=headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
            selectors = [
                '.article-list .article-item',
                'article',
                '.list-group .list-group-item',
                'table tbody tr'
            ]
            
            articles = []
            for selector in selectors:
                articles = soup.select(selector)
                if len(articles) > 0:
                    break
            
            # í…Œì´ë¸” í˜•ì‹ì¸ ê²½ìš°
            if not articles:
                table = soup.find('table')
                if table:
                    rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                    for row in rows[:15]:
                        try:
                            link_tag = row.find('a')
                            if not link_tag:
                                continue
                            
                            title = link_tag.get_text(strip=True)
                            href = link_tag.get('href', '')
                            
                            # ë§í¬ ì²˜ë¦¬
                            if href.startswith('http'):
                                link = href
                            elif href.startswith('/'):
                                link = "https://www.labortoday.co.kr" + href
                            else:
                                link = "https://www.labortoday.co.kr/news/" + href
                            
                            # ë‚ ì§œ ì°¾ê¸°
                            date_text = ''
                            tds = row.find_all('td')
                            for td in tds:
                                text = td.get_text(strip=True)
                                if '.' in text and len(text) < 20:  # ë‚ ì§œ í˜•ì‹ ì¶”ì •
                                    date_text = text
                                    break
                            
                            self.results['labor_news'].append({
                                'title': title,
                                'date': date_text,
                                'link': link,
                                'source': 'ë§¤ì¼ë…¸ë™ë‰´ìŠ¤'
                            })
                        except:
                            continue
            else:
                # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ í˜•ì‹
                for article in articles[:15]:
                    try:
                        # ì œëª©ê³¼ ë§í¬ ì°¾ê¸°
                        title_tag = article.find('a') or article.select_one('.article-title a')
                        if not title_tag:
                            continue
                        
                        title = title_tag.get_text(strip=True)
                        href = title_tag.get('href', '')
                        
                        # ë§í¬ ì²˜ë¦¬
                        if href.startswith('http'):
                            link = href
                        elif href.startswith('/'):
                            link = "https://www.labortoday.co.kr" + href
                        else:
                            link = "https://www.labortoday.co.kr/news/" + href
                        
                        # ë‚ ì§œ ì°¾ê¸°
                        date_tag = article.select_one('.article-date') or article.find('time')
                        date = date_tag.get_text(strip=True) if date_tag else ''
                        
                        self.results['labor_news'].append({
                            'title': title,
                            'date': date,
                            'link': link,
                            'source': 'ë§¤ì¼ë…¸ë™ë‰´ìŠ¤'
                        })
                    except:
                        continue
            
            print(f"  âœ… {len(self.results['labor_news'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def search_bigkinds_news(self, keywords: str = "ì‚°ì—…ì•ˆì „ ì¤‘ëŒ€ì¬í•´"):
        """Bigkindsì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        print(f"ğŸ” Bigkinds ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ (í‚¤ì›Œë“œ: {keywords})...")
        
        if not PLAYWRIGHT_AVAILABLE:
            print("  âš ï¸ Playwright ë¯¸ì„¤ì¹˜ - ê±´ë„ˆëœ€")
            return
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = context.new_page()
                page.set_default_timeout(20000)
                
                print("  â†’ ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì¤‘...")
                try:
                    # í†µí•©ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                    page.goto("https://www.bigkinds.or.kr/v2/news/search.do", 
                             wait_until='domcontentloaded', timeout=15000)
                    page.wait_for_timeout(2000)
                except:
                    print("  âš ï¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                    browser.close()
                    return
                
                # ê²€ìƒ‰ì–´ ì…ë ¥
                try:
                    search_box = page.query_selector('input[type="text"]') or page.query_selector('#search-input')
                    if search_box:
                        search_box.fill(keywords)
                        page.wait_for_timeout(500)
                        
                        # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
                        search_btn = page.query_selector('button[type="submit"]') or page.query_selector('.btn-search')
                        if search_btn:
                            search_btn.click()
                            page.wait_for_timeout(3000)
                        else:
                            # ì—”í„°í‚¤ë¡œ ê²€ìƒ‰
                            search_box.press('Enter')
                            page.wait_for_timeout(3000)
                except:
                    print("  âš ï¸ ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                    browser.close()
                    return
                
                print("  â†’ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ ì¤‘...")
                
                # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
                # BigkindsëŠ” ë™ì  ë¡œë”©ì´ë¯€ë¡œ ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                selectors = [
                    '.news-item',
                    '.search-result-item',
                    'article',
                    '.list-item',
                    '[class*="result"]'
                ]
                
                results = []
                for selector in selectors:
                    try:
                        results = page.query_selector_all(selector)
                        if len(results) > 0:
                            break
                    except:
                        continue
                
                count = 0
                for result in results[:10]:  # ìµœê·¼ 10ê±´ë§Œ
                    try:
                        # ì œëª© ì°¾ê¸°
                        title = ''
                        title_selectors = ['h3', 'h4', '.title', 'a', 'strong']
                        for ts in title_selectors:
                            title_elem = result.query_selector(ts)
                            if title_elem:
                                title = title_elem.inner_text().strip()
                                if len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©
                                    break
                        
                        # ë§í¬ ì°¾ê¸°
                        link = ''
                        link_elem = result.query_selector('a')
                        if link_elem:
                            href = link_elem.get_attribute('href') or ''
                            if href.startswith('http'):
                                link = href
                            elif href.startswith('/'):
                                link = "https://www.bigkinds.or.kr" + href
                        
                        # ë‚ ì§œ ì°¾ê¸°
                        date = ''
                        date_selectors = ['.date', 'time', 'span', '.info']
                        for ds in date_selectors:
                            date_elem = result.query_selector(ds)
                            if date_elem:
                                date_text = date_elem.inner_text().strip()
                                # ë‚ ì§œ í˜•ì‹ í™•ì¸ (YYYY-MM-DD, YYYY.MM.DD ë“±)
                                if any(char in date_text for char in ['-', '.', '/']) and len(date_text) < 15:
                                    date = date_text
                                    break
                        
                        # ì–¸ë¡ ì‚¬ ì°¾ê¸°
                        source = 'Bigkinds'
                        source_selectors = ['.source', '.press', '.media']
                        for ss in source_selectors:
                            source_elem = result.query_selector(ss)
                            if source_elem:
                                source_text = source_elem.inner_text().strip()
                                if source_text:
                                    source = source_text
                                    break
                        
                        # ì•ˆì „ë³´ê±´ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                        safety_keywords = ['ì•ˆì „', 'ì‚°ì¬', 'ì¤‘ëŒ€ì¬í•´', 'ì¬í•´', 'ì‚¬ê³ ', 'ë³´ê±´', 'ìœ„í—˜']
                        if title and any(kw in title for kw in safety_keywords):
                            self.results['bigkinds_news'].append({
                                'title': title,
                                'date': date or self.today,
                                'link': link,
                                'source': source
                            })
                            count += 1
                    except:
                        continue
                
                browser.close()
            
            print(f"  âœ… {len(self.results['bigkinds_news'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
    
    def search_additional_news(self):
        """ì¶”ê°€ ì–¸ë¡ ê¸°ì‚¬ ê²€ìƒ‰"""
        print("ğŸ” ì¶”ê°€ ì–¸ë¡ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
        print("  â„¹ï¸  Bigkinds ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ë ¤ë©´ search_bigkinds_news() ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì„¸ìš”")
        print("  â­ï¸  ì¶”ê°€ ë‰´ìŠ¤ ê²€ìƒ‰ì€ ë³„ë„ êµ¬í˜„ í•„ìš”")
    
    def run_all_scrapers(self):
        """ëª¨ë“  ìŠ¤í¬ë˜í¼ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸ¤– ì¼ì¼ ë™í–¥ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {self.today}")
        print(f"{'='*60}\n")
        
        self.scrape_moel_press_release()
        time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        self.scrape_kosha_with_playwright()
        time.sleep(2)
        
        self.scrape_major_accidents()
        time.sleep(2)
        
        self.scrape_labor_news()
        time.sleep(2)
        
        self.search_additional_news()
        
        print(f"\n{'='*60}")
        print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"{'='*60}\n")
        
        return self.results
    
    def get_summary(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½"""
        total = sum(len(v) for v in self.results.values())
        summary = {
            'total': total,
            'by_source': {k: len(v) for k, v in self.results.items()}
        }
        return summary


if __name__ == "__main__":
    scraper = SafetyNewsScraper()
    results = scraper.run_all_scrapers()
    summary = scraper.get_summary()
    
    print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
    print(f"  ì´ {summary['total']}ê±´")
    for source, count in summary['by_source'].items():
        print(f"  - {source}: {count}ê±´")
